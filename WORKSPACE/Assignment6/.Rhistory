training_set_scaled[, 1:2] <- predict(sc, training_set[, 1:2])
test_set_scaled <- test_set
test_set_scaled[, 1:2] <- predict(sc, test_set[, 1:2])
# Create polynomial features
poly_features <- function(X) {
data.frame(
X1 = X[, 1],
X2 = X[, 2],
X1_2 = X[, 1]^2,
X2_2 = X[, 2]^2,
X1_X2 = X[, 1] * X[, 2]
)
}
X_poly_train <- poly_features(training_set_scaled[, 1:2])
X_poly_test <- poly_features(test_set_scaled[, 1:2])
# Fit the logistic regression model
classifier <- glm(training_set_scaled$Purchased ~ ., data = X_poly_train, family = binomial)
# Generate a grid of values for plotting
x1_seq <- seq(min(dataset$Age), max(dataset$Age), by = 1)
x2_seq <- seq(min(dataset$EstimatedSalary), max(dataset$EstimatedSalary), by = 100)
grid <- expand.grid(Age = x1_seq, EstimatedSalary = x2_seq)
# Scale the grid
grid_scaled <- predict(sc, grid)
# Apply polynomial features to the scaled grid
X_poly_grid_scaled <- poly_features(grid_scaled)
# Predict the outcomes for the grid
y_grid_pred <- predict(classifier, newdata = X_poly_grid_scaled, type = 'response')
y_grid_class <- ifelse(y_grid_pred > 0.5, 1, 0)
# Add the predictions to the grid
grid$Purchased <- factor(y_grid_class)
# Plot the decision boundary
ggplot() +
geom_tile(data = grid, aes(x = Age, y = EstimatedSalary, fill = Purchased), alpha = 0.4) +
scale_fill_manual(values = c('red', 'green')) +
geom_point(data = test_set_scaled, aes(x = Age, y = EstimatedSalary, color = Purchased), size = 2) +
scale_color_manual(values = c('red', 'green')) +
labs(title = 'Logistic Regression (Test set)',
x = 'Age (scaled)',
y = 'EstimatedSalary (scaled)') +
theme_minimal()
# Load libraries
library(ggplot2)
library(caret)
# Load data
dataset <- read.csv('Social_Network_Ads.csv')
# Convert Purchased to factor
dataset$Purchased <- factor(dataset$Purchased)
# Select relevant columns
dataset <- dataset[, c("Age", "EstimatedSalary", "Purchased")]
# Train-test split
set.seed(123)
splitIndex <- createDataPartition(dataset$Purchased, p = 0.75, list = FALSE)
training_set <- dataset[splitIndex, ]
test_set <- dataset[-splitIndex, ]
# Feature scaling
sc <- preProcess(training_set[, 1:2], method = c("center", "scale"))
training_set_scaled <- training_set
training_set_scaled[, 1:2] <- predict(sc, training_set[, 1:2])
test_set_scaled <- test_set
test_set_scaled[, 1:2] <- predict(sc, test_set[, 1:2])
# Polynomial features
poly_features <- function(X) {
data.frame(
X1 = X[, 1],
X2 = X[, 2],
X1_2 = X[, 1]^2,
X2_2 = X[, 2]^2,
X1_X2 = X[, 1] * X[, 2]
)
}
X_poly_train <- poly_features(training_set_scaled[, 1:2])
X_poly_test <- poly_features(test_set_scaled[, 1:2])
# Fit logistic regression
classifier <- glm(training_set_scaled$Purchased ~ ., data = X_poly_train, family = binomial)
# PHASE 1: Generate Grid for X1 and X2 in original scale
x1_seq <- seq(min(dataset$Age), max(dataset$Age), by = 1)
x2_seq <- seq(min(dataset$EstimatedSalary), max(dataset$EstimatedSalary), by = 100)
grid <- expand.grid(Age = x1_seq, EstimatedSalary = x2_seq)
# Scale the features
grid_scaled <- grid
names(grid_scaled) <- c("Age", "EstimatedSalary") # Ensure column names match
grid_scaled <- predict(sc, grid_scaled)
# Apply polynomial features again after scaling
X_poly_grid_scaled <- poly_features(grid_scaled)
# Predict
y_grid_pred <- predict(classifier, newdata = X_poly_grid_scaled, type = 'response')
y_grid_class <- ifelse(y_grid_pred > 0.5, 1, 0)
# PHASE 3: Plotting
grid$Purchased <- factor(y_grid_class)
# Plot decision boundary
ggplot() +
geom_tile(data = grid, aes(x = Age, y = EstimatedSalary, fill = Purchased), alpha = 0.4) +
scale_fill_manual(values = c('red', 'green')) +
geom_point(data = dataset, aes(x = Age, y = EstimatedSalary, color = Purchased), size = 2) +
scale_color_manual(values = c('red', 'green')) +
labs(title = 'Logistic Regression',
x = 'Age',
y = 'EstimatedSalary') +
theme_minimal()
# Load libraries
library(ggplot2)
library(caret)
# Load data
dataset <- read.csv('Social_Network_Ads.csv')
# Convert Purchased to factor
dataset$Purchased <- factor(dataset$Purchased)
# Select relevant columns
dataset <- dataset[, c("Age", "EstimatedSalary", "Purchased")]
# Train-test split
set.seed(123)
splitIndex <- createDataPartition(dataset$Purchased, p = 0.75, list = FALSE)
training_set <- dataset[splitIndex, ]
test_set <- dataset[-splitIndex, ]
# Feature scaling
sc <- preProcess(training_set[, 1:2], method = c("center", "scale"))
training_set_scaled <- training_set
training_set_scaled[, 1:2] <- predict(sc, training_set[, 1:2])
test_set_scaled <- test_set
test_set_scaled[, 1:2] <- predict(sc, test_set[, 1:2])
# Polynomial features
poly_features <- function(X) {
data.frame(
X1 = X[, 1],
X2 = X[, 2],
X1_2 = X[, 1]^2,
X2_2 = X[, 2]^2,
X1_X2 = X[, 1] * X[, 2]
)
}
# Apply polynomial features to scaled training and test set
X_poly_train <- poly_features(training_set_scaled[, 1:2])
X_poly_test <- poly_features(test_set_scaled[, 1:2])
# Fit logistic regression
classifier <- glm(training_set_scaled$Purchased ~ ., data = X_poly_train, family = binomial)
# PHASE 1: Generate Grid for X1 and X2 in original scale
x1_seq <- seq(min(dataset$Age), max(dataset$Age), by = 1)
x2_seq <- seq(min(dataset$EstimatedSalary), max(dataset$EstimatedSalary), by = 100)
grid <- expand.grid(Age = x1_seq, EstimatedSalary = x2_seq)
# Scale the grid using the same scaling parameters
grid_scaled <- predict(sc, grid)
# Apply polynomial features again after scaling
X_poly_grid_scaled <- poly_features(grid_scaled)
# Predict
y_grid_pred <- predict(classifier, newdata = X_poly_grid_scaled, type = 'response')
y_grid_class <- ifelse(y_grid_pred > 0.5, 1, 0)
# PHASE 3: Plotting
grid$Purchased <- factor(y_grid_class)
# Plot decision boundary
ggplot() +
geom_tile(data = grid, aes(x = Age, y = EstimatedSalary, fill = Purchased), alpha = 0.4) +
scale_fill_manual(values = c('red', 'green')) +
geom_point(data = dataset, aes(x = Age, y = EstimatedSalary, color = Purchased), size = 2) +
scale_color_manual(values = c('red', 'green')) +
labs(title = 'Logistic Regression Decision Boundary',
x = 'Age',
y = 'EstimatedSalary') +
theme_minimal()
#-------------------------------------------------------------------------------
# 1. Adding degree-2 polynomial features: age^2, salary^2, and age * salary
#-------------------------------------------------------------------------------
# Loading the dataset
data <- read.csv('Social_Network_Ads.csv')
# Creating the degree-2 features
data$age2 <- data$Age^2
data$salary2 <- data$EstimatedSalary^2
data$age_salary <- data$Age * data$EstimatedSalary
# Encoding the target variable
data$Purchased <- as.factor(data$Purchased)
#-------------------------------------------------------------------------------
# 2. Splitting the data: 75% for training, 25% for testing. Setting seed for reproducibility.
#-------------------------------------------------------------------------------
# Splitting the dataset into training and test sets
library(caTools)
set.seed(123)  # Setting seed for consistency
split <- sample.split(data$Purchased, SplitRatio = 0.75)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
#-------------------------------------------------------------------------------
# 3. Feature scaling: Scaling age, salary, age^2, salary^2, and age*salary.
#-------------------------------------------------------------------------------
# Scaling the features
train_scaled_features <- scale(train_data[, c(1:2, 4:6)])  # Scaling the relevant columns
train_data[, c(1:2, 4:6)] <- train_scaled_features
test_data[, c(1:2, 4:6)] <- scale(test_data[, c(1:2, 4:6)],
center = attr(train_scaled_features, 'scaled:center'),
scale = attr(train_scaled_features, 'scaled:scale'))
#-------------------------------------------------------------------------------
# 4. Training the logistic regression model and evaluating it on the test set
#-------------------------------------------------------------------------------
# Logistic Regression model
model <- glm(formula = Purchased ~ .,
family = binomial,
data = train_data)
# Predicting the test set results
pred_probs <- predict(model, type = 'response', newdata = test_data)
predictions <- as.factor(ifelse(pred_probs > 0.5, 1, 0))
# Evaluating the model's performance
library(caret)
conf_matrix <- confusionMatrix(predictions, test_data$Purchased)
print(conf_matrix$table)
print(conf_matrix$overall['Accuracy'])
#-------------------------------------------------------------------------------
# 5. Visualizing the results for both training and test sets
#-------------------------------------------------------------------------------
# Visualizing the Training Set Results
train_set <- train_data
X1_range <- seq(min(train_set[, 1]) - 1, max(train_set[, 1]) + 1, by = 0.01)
X2_range <- seq(min(train_set[, 2]) - 1, max(train_set[, 2]) + 1, by = 0.01)
grid <- expand.grid(Age = X1_range, EstimatedSalary = X2_range)
# Adding polynomial features to the grid
grid$age2 <- grid$Age^2
grid$salary2 <- grid$EstimatedSalary^2
grid$age_salary <- grid$Age * grid$EstimatedSalary
# Scaling the grid features based on training data
grid[, c(1:5)] <- scale(grid[, c(1:5)],
center = attr(train_scaled_features, 'scaled:center'),
scale = attr(train_scaled_features, 'scaled:scale'))
# Predicting over the grid
prob_grid <- predict(model, type = 'response', newdata = grid)
grid_predictions <- as.factor(ifelse(prob_grid > 0.5, 1, 0))
# Plotting the decision boundary for training set
plot(NULL,
main = 'Logistic Regression (Training Set)',
xlab = 'Age (Scaled)', ylab = 'Estimated Salary (Scaled)',
xlim = range(X1_range), ylim = range(X2_range))
points(grid, pch = 20, col = c('tomato', 'springgreen3')[grid_predictions])
points(train_set, pch = 21, bg = c('red3', 'green4')[train_set$Purchased])
# (Optional) Repeat similar visualization for Test Set
#-------------------------------------------------------------------------------
# 1. Add degree-2 polynomial features. The added features should contain degree-2 age,
# degree-2 salary, and age times salary
#-------------------------------------------------------------------------------
#Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
# add age degree-2
dataset$age2 = dataset$Age^2
# add salary degree-2
dataset$salary2 = dataset$EstimatedSalary^2
# add age times salary
dataset$age_salary = dataset$Age * dataset$EstimatedSalary
# Encoding categorical data
dataset$Purchased = as.factor(dataset$Purchased)
#-------------------------------------------------------------------------------
# 2. 25% of the data should go to the test set. In addition, random_state must be set to 0
# in Python and seed must be set to 123 in R.
#-------------------------------------------------------------------------------
# Splitting the dataset into Training and Test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
#-------------------------------------------------------------------------------
# 3. Feature scaling is required for both Python and R.
#-------------------------------------------------------------------------------
# Feature Scaling
training_scaled_cols = scale(training_set[, c(1:2, 4:6)])
training_set[, c(1:2, 4:6)] = training_scaled_cols
test_set[, c(1:2, 4:6)] = scale(test_set[, c(1:2, 4:6)],
center = attr(training_scaled_cols, 'scaled:center'),
scale = attr(training_scaled_cols, 'scaled:scale'))
#-------------------------------------------------------------------------------
# 4. Train your model based on the training set. Then, print out the confusion matrix and
# accuracy based on the test set.
#-------------------------------------------------------------------------------
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set)
y_pred = as.factor(ifelse(prob_pred > 0.5, 1, 0))
# Showing the confusion matrix and accuracy
library(caret)
cm = confusionMatrix(y_pred, test_set$Purchased)
print(cm$table)
print(cm$overall['Accuracy'])
#-------------------------------------------------------------------------------
# 5. A training set plot and a test set plot must be generated in each programming language.
# The style of the plots should be identical to the one used in class. In each plot:
# • The horizontal axis should be scaled age and the vertical axis should be scaled
# salary.
# • For background, use light red to represent the predicted region of “not purchased”
# and use light green to represent the predicted region of “purchased”.
# • Use red dots to represent “not purchased” observations and use green dots to
# represent “purchased” observations.
# • Have proper title and axis labels
#-------------------------------------------------------------------------------
# Visualizing the Training set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(Age = X1, EstimatedSalary = X2)
# add features
grid_set$age2 = grid_set$Age^2
grid_set$salary2 = grid_set$EstimatedSalary^2
grid_set$age_salary = grid_set$Age * grid_set$EstimatedSalary
# scale the features only
grid_set[, c(1:5)] = scale(grid_set[, c(1:5)],
center = attr(training_scaled_cols, 'scaled:center'),
scale = attr(training_scaled_cols, 'scaled:scale'))
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = as.factor(ifelse(prob_set > 0.5, 1, 0))
plot(NULL,
main = 'Logistic Regression (Test set)',
xlab = 'Age (Scaled)', ylab = 'Estimated Salary (Scaled)',
xlim = range(X1), ylim = range(X2))
points(grid_set, pch = 20, col = c('tomato', 'springgreen3')[y_grid])
points(set, pch = 21, bg = c('red3', 'green4')[set$Purchased])
#-------------------------------------------------------------------------------
# 1. Add degree-2 polynomial features. The added features should contain degree-2 age,
# degree-2 salary, and age times salary
#-------------------------------------------------------------------------------
#Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
# add age degree-2
dataset$age2 = dataset$Age^2
# add salary degree-2
dataset$salary2 = dataset$EstimatedSalary^2
# add age times salary
dataset$age_salary = dataset$Age * dataset$EstimatedSalary
# Encoding categorical data
dataset$Purchased = as.factor(dataset$Purchased)
#-------------------------------------------------------------------------------
# 2. 25% of the data should go to the test set. In addition, random_state must be set to 0
# in Python and seed must be set to 123 in R.
#-------------------------------------------------------------------------------
# Splitting the dataset into Training and Test set
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
#-------------------------------------------------------------------------------
# 3. Feature scaling is required for both Python and R.
#-------------------------------------------------------------------------------
# Feature Scaling
training_scaled_cols = scale(training_set[, c(1:2, 4:6)])
training_set[, c(1:2, 4:6)] = training_scaled_cols
test_set[, c(1:2, 4:6)] = scale(test_set[, c(1:2, 4:6)],
center = attr(training_scaled_cols, 'scaled:center'),
scale = attr(training_scaled_cols, 'scaled:scale'))
#-------------------------------------------------------------------------------
# 4. Train your model based on the training set. Then, print out the confusion matrix and
# accuracy based on the test set.
#-------------------------------------------------------------------------------
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set)
y_pred = as.factor(ifelse(prob_pred > 0.5, 1, 0))
# Showing the confusion matrix and accuracy
library(caret)
cm = confusionMatrix(y_pred, test_set$Purchased)
print(cm$table)
print(cm$overall['Accuracy'])
#-------------------------------------------------------------------------------
# 5. A training set plot and a test set plot must be generated in each programming language.
# The style of the plots should be identical to the one used in class. In each plot:
# • The horizontal axis should be scaled age and the vertical axis should be scaled
# salary.
# • For background, use light red to represent the predicted region of “not purchased”
# and use light green to represent the predicted region of “purchased”.
# • Use red dots to represent “not purchased” observations and use green dots to
# represent “purchased” observations.
# • Have proper title and axis labels
#-------------------------------------------------------------------------------
# Visualizing the Training set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(Age = X1, EstimatedSalary = X2)
# add features
grid_set$age2 = grid_set$Age^2
grid_set$salary2 = grid_set$EstimatedSalary^2
grid_set$age_salary = grid_set$Age * grid_set$EstimatedSalary
# scale the features only
grid_set[, c(1:5)] = scale(grid_set[, c(1:5)],
center = attr(training_scaled_cols, 'scaled:center'),
scale = attr(training_scaled_cols, 'scaled:scale'))
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = as.factor(ifelse(prob_set > 0.5, 1, 0))
plot(NULL,
main = 'Logistic Regression (Test set)',
xlab = 'Age (Scaled)', ylab = 'Estimated Salary (Scaled)',
xlim = range(X1), ylim = range(X2))
points(grid_set, pch = 20, col = c('tomato', 'springgreen3')[y_grid])
points(set, pch = 21, bg = c('red3', 'green4')[set$Purchased])
# logistic_regression_polynomial.R
#
# This script:
# 1. Loads Social_Network_Ads.csv
# 2. Adds degree‑2 polynomial features: age², salary², age×salary
# 3. Splits into 75% train / 25% test (seed = 123)
# 4. Scales all predictors
# 5. Trains a logistic regression
# 6. Prints confusion matrix & accuracy on test set
# 7. Plots decision regions & points for both training and test sets
#-------------------------------------------------------------------------------
# 0. Libraries
#-------------------------------------------------------------------------------
library(caTools)
library(caret)
#-------------------------------------------------------------------------------
# 1. Load data & feature engineering
#-------------------------------------------------------------------------------
dataset <- read.csv("Social_Network_Ads.csv")
dataset$age2        <- dataset$Age^2
dataset$salary2     <- dataset$EstimatedSalary^2
dataset$age_salary  <- dataset$Age * dataset$EstimatedSalary
dataset$Purchased   <- as.factor(dataset$Purchased)
#-------------------------------------------------------------------------------
# 2. Split into train/test
#-------------------------------------------------------------------------------
set.seed(123)
split      <- sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set     <- subset(dataset, split == FALSE)
#-------------------------------------------------------------------------------
# 3. Feature scaling (keep centers & scales for later)
#-------------------------------------------------------------------------------
# columns: 1=Age, 2=EstimatedSalary, 4=age2, 5=salary2, 6=age_salary
training_scaled_cols <- scale(training_set[, c(1,2,4,5,6)])
centers <- attr(training_scaled_cols, "scaled:center")
scales  <- attr(training_scaled_cols, "scaled:scale")
training_set[, c(1,2,4,5,6)] <- training_scaled_cols
test_set[, c(1,2,4,5,6)]     <- scale(
test_set[, c(1,2,4,5,6)],
center = centers,
scale  = scales
)
#-------------------------------------------------------------------------------
# 4. Train logistic regression
#-------------------------------------------------------------------------------
classifier <- glm(
formula = Purchased ~ .,
family  = binomial,
data    = training_set
)
#-------------------------------------------------------------------------------
# 5. Evaluate on test set
#-------------------------------------------------------------------------------
prob_pred <- predict(classifier,
type    = "response",
newdata = test_set)
y_pred <- as.factor(ifelse(prob_pred > 0.5, 1, 0))
cm <- confusionMatrix(y_pred, test_set$Purchased)
print(cm$table)
print(cm$overall["Accuracy"])
#-------------------------------------------------------------------------------
# 6. Helper function to plot decision regions
#-------------------------------------------------------------------------------
plot_decision <- function(dataset_scaled, title_text) {
# Extract scaled coords
X1 <- seq(min(dataset_scaled[,1]) - 1,
max(dataset_scaled[,1]) + 1,
by = 0.01)
X2 <- seq(min(dataset_scaled[,2]) - 1,
max(dataset_scaled[,2]) + 1,
by = 0.01)
# Build a grid of scaled points
grid_coords <- expand.grid(X1, X2)
names(grid_coords) <- c("x1","x2")
# Un‑scale back to raw Age & Salary
raw_age <- grid_coords$x1 * scales["Age"] +
centers["Age"]
raw_sal <- grid_coords$x2 * scales["EstimatedSalary"] +
centers["EstimatedSalary"]
# Re‑compute raw polynomial features
raw_grid <- data.frame(
Age             = raw_age,
EstimatedSalary = raw_sal,
age2            = raw_age^2,
salary2         = raw_sal^2,
age_salary      = raw_age * raw_sal
)
# Re‑scale all five columns
grid_for_pred <- as.data.frame(
scale(raw_grid,
center = centers,
scale  = scales)
)
# Predict on the grid
prob_set <- predict(classifier,
type    = "response",
newdata = grid_for_pred)
y_grid <- as.factor(ifelse(prob_set > 0.5, 1, 0))
# Plot background
plot(NULL,
main = title_text,
xlab = "Age (Scaled)",
ylab = "Estimated Salary (Scaled)",
xlim = range(X1),
ylim = range(X2))
points(grid_coords$x1,
grid_coords$x2,
pch = 20,
col = c("tomato","springgreen3")[y_grid])
# Overlay the actual datapoints
points(dataset_scaled[,1],
dataset_scaled[,2],
pch = 21,
bg  = c("red3","green4")[dataset_scaled$Purchased])
}
#-------------------------------------------------------------------------------
# 7. Visualize Training set
#-------------------------------------------------------------------------------
plot_decision(training_set, "Logistic Regression (Training set)")
#-------------------------------------------------------------------------------
# 8. Visualize Test set
#-------------------------------------------------------------------------------
plot_decision(test_set, "Logistic Regression (Test set)")
